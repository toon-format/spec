% TOON JOSS Paper Bibliography
% Required citations for JOSS submission

% Related Work: Prompt Compression
@inproceedings{jiang_llmlingua_2023,
  title     = {{LLMLingua}: {Compressing} Prompts for Accelerated Inference
               of Large Language Models},
  author    = {Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and
               Yang, Yuqing and Qiu, Lili},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods
               in Natural Language Processing},
  year      = {2023},
  doi       = {10.18653/v1/2023.emnlp-main.825}
}

% Related Work: Constrained Decoding
@article{willard_efficient_2023,
  title   = {Efficient Guided Generation for Large Language Models},
  author  = {Willard, Brandon T. and Louf, R{\'e}mi},
  journal = {arXiv preprint arXiv:2307.09702},
  year    = {2023},
  doi     = {10.48550/arXiv.2307.09702}
}

% Independent Validation
@article{masciari_toon_2025,
  title   = {Evaluating {TOON} for Token-Efficient Structured Data
             in Large Language Models},
  author  = {Masciari, Elio and others},
  journal = {arXiv preprint},
  year    = {2025},
  note    = {Independent benchmarking study confirming 26-49\% token reduction
             and 77-88\% lower carbon emissions across 8 LLMs}
}

% Methodology: Tokenizer
@software{tiktoken_2023,
  title   = {tiktoken: {BPE} tokeniser for use with {OpenAI's} models},
  author  = {{OpenAI}},
  year    = {2023},
  url     = {https://github.com/openai/tiktoken},
  note    = {cl100k\_base encoding used for token measurements}
}
